---
question: Почему использование L1 регуляризации для линейных моделей может привести к отбору признаков (в отличие от L2 регуляризации)?
module: Теория
---

L1-регуляризация (например, Lasso) добавляет к функции ошибки сумму модулей коэффициентов (|w|). Особенность L1 в том, что она может занулить некоторые веса полностью. Оптимизируя с таким штрафом, модель стремится обнулить наименее значимые признаки, чтобы снизить суммарный модуль весов. Геометрически ограничения L1 образуют "ромб" в пространстве параметров, у которого вершины на осях – поэтому оптимум часто лежит на одной из осей, где какой-то вес равен нулю. В итоге L1 дает <strong>разреженное решение, многие коэффициенты становятся точно 0</strong>. Это фактически выполняет отбор признаков: признаки с нулевыми весами исключаются из модели.<br>L2-регуляризация (ридж-регрессия) напротив использует сумму квадратов весов. Она стремится только "сжать" веса к нулю, но не делает их ровно нулевыми (оптимум при L2 – гладко смещён к нулю без резких углов). Поэтому L2 уменьшает влияние слабых признаков, но редко обнуляет их полностью. Именно поэтому L1 может отобрать небольшой поднабор важных признаков, а L2 скорее будет удерживать все понемногу.
