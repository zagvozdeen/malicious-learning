---
question: Какие методы машинного обучения, реализованные в scikit-learn имеют адекватную поддержку многоклассовй классификации, а какие костыльную (через стратегии один против всех / один против одного).
module: Практика
---

**Нативную поддержку многоклассовости** имеют модели, которые по своей природе умеют работать с более чем двумя классами. Например, **деревья решений и случайные леса** напрямую делят данные на множество классов (в листьях могут быть не только два варианта). **Наивный Байес** (GaussianNB, MultinomialNB) тоже естественно работает с любым числом классов, вычисляя апостериорные вероятности для каждого. **k-ближайших соседей (k-NN)** - при классификации просто смотрит, какой класс чаще среди соседей, и это работает для любого количества классов без дополнительных ухищрений. **Логистическая регрессия** в режиме multi_class='multinomial' фактически обучает один модельный вектор, который сразу различает все классы (softmax на выходе) - это тоже истинная многоклассовая модель.

**"Костыльную" поддержку (через разбиение задачи на бинарные подзадачи)** имеют алгоритмы, которые изначально разработаны для бинарной классификации. Классический пример - **SVM (опорные векторы)**: в scikit-learn реализация SVC по умолчанию использует стратегию **один против одного** (OvO), обучая несколько бинарных SVM для каждой пары классов, а LinearSVC - стратегию **один против всех** (OvA), обучая отдельный классификатор на каждый класс против остальных. То есть SVM сам по себе решает только да/нет, и многоклассовость достигается комбинацией нескольких таких моделей. Аналогично, **логистическая регрессия** по умолчанию (параметр multi_class='auto' с solver='liblinear') сводится к набору бинарных задач OvA, если явно не указать использовать мультиномиальный вариант. Многие **линейные модели (например, Perceptron)** в sklearn тоже используют OvA для нескольких классов. В итоге методы, не имеющие прямой многоклассовой формулы, реализованы через обучение нескольких двуклассовых моделей.
