---
question: Предположите, почему ансамблевые модели, основанные на деревьях решений, работают лучше на табличных данных чем Нейронные сети (можно вдохновиться [этой статьёй](https://arxiv.org/pdf/2207.08815.pdf))
module: Теория
---

Ансамбли на деревьях (Random Forest, XGBoost) часто лучше подходят для **табличных данных**, потому что их “встроенные привычки” (inductive bias) удачно совпадают с тем, как устроены таблицы:

- **В таблицах много “мусорных”/слабополезных признаков.** Деревья при разбиениях легко их игнорируют, а MLP-подобные нейросети заметно сильнее “страдают” от неинформативных фич (разрыв в качестве растёт, когда таких фич становится больше).
- **Табличные данные “не любят повороты” признакового пространства.** У каждого столбца обычно есть отдельный смысл (“возраст”, “вес”), а случайные линейные смеси этих признаков могут портить структуру. Авторы показывают, что “ротационная инвариантность” (типичная для обучения MLP) здесь скорее вредна, и связывают это с тем, что при таком свойстве алгоритму сложнее отсеивать нерелевантные признаки.
- **Зависимости в табличных задачах часто “неровные”, со скачками и порогами.** Деревья естественно учат кусочно-постоянные правила (условия вида “если X > t…”), а нейросети чаще смещены к более “гладким” решениям и хуже подхватывают такие “ступеньки”.
- **Практика подбора гиперпараметров:** даже при большом бюджете рандом-поиска деревья остаются сильнее, а каждая попытка для нейросети обычно ещё и дороже по времени.

(В самой статье это сводится к трём “челленджам” для табличных нейросетей: быть устойчивыми к неинформативным фичам, сохранять ориентацию данных и уметь легко учить нерегулярные функции.)
