---
question: Сравните хотя бы 2 метода градиентного бустинга (как с т.з. «внутренностей», так и с точки зрения применения на практике)
module: Теория
---

Сравним популярные реализации градиентного бустинга: например, **XGBoost**, **LightGBM** и **CatBoost**:

- **XGBoost (Extreme Gradient Boosting):** использует построчный (level-wise) рост деревьев, тщательно просматривая возможные разбиения. Имеет продвинутые возможности регуляризации (L1, L2 на листьях), встроенную обработку пропусков. XGBoost вычисляет точные решения или по желанию использует гистограммы. На практике XGBoost хорошо работает "из коробки", хотя может быть медленнее, чем некоторые новые реализации, особенно на очень больших данных. Он стабилен и даёт предсказуемое качество, требует явного кодирования категорий (сам их не обрабатывает).

- **LightGBM:** акцент на скорость и память. Вместо перебора всех порогов признаков, LightGBM использует гистограммный подход (признаки распределяются по бинам). Кроме того, он растит деревья "по листьям" (leaf-wise): в каждой итерации расширяет ту ветвь, где выигрыш по метрике максимален, что может приводить к более глубоким деревьям. Это очень ускоряет обучение на больших наборах данных. LightGBM часто быстрее XGBoost при сопоставимом качестве. Однако из-за агрессивного роста по листьям есть риск переобучиться, поэтому важны ограничивающие параметры (макс. глубина или мин. размер листа). Категориальные признаки он умеет обрабатывать через специальный способ (сортировка по частоте и поиск разбиения), но часто применяют и классическое one-hot при необходимости.

- **CatBoost:** ключевая фишка – встроенная обработка категориальных признаков с помощью методик, избегающих утечки (например, кодирование категориальных значений по целевому признаку с использованием случайных перестановок (чтобы избежать утечки информации)). Он строит так называемые симметричные деревья (каждый уровень сбалансирован по признакам), что облегчает использование на GPU и стабилизирует структуру модели. По скорости CatBoost обычно сравним с LightGBM (иногда медленнее на числовых данных из-за доп. расчётов для категорий). Зато на данных с множеством категорий он, как правило, показывает превосходное качество без ручной подготовки. В применении CatBoost удобен тем, что требует минимум преобразований данных и довольно устойчив к настройкам (меньше параметров надо крутить для начала).

На практике выбор между ними часто зависит от данных и условий: XGBoost – надёжный выбор, особенно если нужны регуляризация и проверенный временем алгоритм. LightGBM – когда важны скорость и ресурсная эффективность, особенно на больших выборках. CatBoost – когда много категориальных признаков или хочется получить высокий результат без долгого подбора кодирования для категорий. По качеству все три, как правило, дают близкие результаты, но в конкретных задачах один может сработать лучше других. Кроме этих, есть и другие бустинги (например, оригинальный AdaBoost, HistGradientBoosting в scikit-learn), но XGBoost/LightGBM/CatBoost – наиболее популярны сегодня.
