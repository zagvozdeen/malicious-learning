---
question: Ключевые шаги модели линейной регрессии. Обоснование функции потерь
module: Теория
---

Модель линейной регрессии включает следующие ключевые шаги:<br><ol><br>  <li><strong>Задаём линейную форму модели:</strong> предполагается, что целевая переменная связана с признаками линейно: y_pred = w0 + w1*x1 + ... + wn*xn. То есть, мы ищем коэффициенты w, при которых прямая (или гиперплоскость) наилучшим образом приближает исходные данные.</li><br>  <li><strong>Выбираем функцию потерь:</strong> обычно используется <strong>среднеквадратичная ошибка (MSE)</strong>, то есть средний квадрат разницы между предсказанием и истинным значением. Эта функция потерь удобно дифференцируется и даёт больший штраф за сильные отклонения, что способствует поиску "наилучшей" по среднему квадратическому критерию прямой. Кроме того, MSE соответствует предположению о нормальном (гауссовском) распределении ошибок: выбор MSE эквивалентен методу максимального правдоподобия при таких ошибках.</li><br>  <li><strong>Оптимизируем параметры (обучение):</strong> цель – найти коэффициенты w, минимизирующие функцию потерь на обучающих данных. В случае линейной регрессии задача сведения к решению нормальных уравнений (X<sup>T</sup>Xw = X<sup>T</sup>y) или решается численно методом градиентного спуска. В итоге получаем набор весов w, при котором прямая наилучшим образом приближает тренировочные точки.</li><br></ol><br>Почему берут именно MSE? Эта функция потерь обеспечивает единственное минимальное решение (выпуклая парабола по параметрам) и математически удобна – её производная по каждому параметру проста, что позволяет легко найти оптимальные w. К тому же, как упомянуто, она имеет статистическое обоснование (минимизация MSE равносильна поиску МНК-оценки при нормальных шумах).
