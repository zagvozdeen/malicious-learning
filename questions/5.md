---
question: Ключевые шаги модели линейной регрессии. Обоснование функции потерь
module: Теория
---

Модель линейной регрессии включает следующие ключевые шаги:

1. **Задаём линейную форму модели:** предполагается, что целевая переменная связана с признаками линейно: y_pred = w0 + w1*x1 + ... + wn*xn. То есть, мы ищем коэффициенты w, при которых прямая (или гиперплоскость) наилучшим образом приближает исходные данные.

1. **Выбираем функцию потерь:** обычно используется **среднеквадратичная ошибка (MSE)**, то есть средний квадрат разницы между предсказанием и истинным значением. Эта функция потерь удобно дифференцируется и даёт больший штраф за сильные отклонения, что способствует поиску "наилучшей" по среднему квадратическому критерию прямой. Кроме того, MSE соответствует предположению о нормальном (гауссовском) распределении ошибок: выбор MSE эквивалентен методу максимального правдоподобия при таких ошибках.

1. **Оптимизируем параметры (обучение):** цель – найти коэффициенты w, минимизирующие функцию потерь на обучающих данных. В случае линейной регрессии задача сведения к решению нормальных уравнений (X^{T}Xw = X^{T}y) или решается численно методом градиентного спуска. В итоге получаем набор весов w, при котором прямая наилучшим образом приближает тренировочные точки.

Почему берут именно MSE? Эта функция потерь обеспечивает единственное минимальное решение (выпуклая парабола по параметрам) и математически удобна – её производная по каждому параметру проста, что позволяет легко найти оптимальные w. К тому же, как упомянуто, она имеет статистическое обоснование (минимизация MSE равносильна поиску МНК-оценки при нормальных шумах).
