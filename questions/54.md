---
question: Как связаны разные гиперпараметры деревьев решений с возможностью переобучится / недообучится?
module: Практика
---

Гиперпараметры дерева решений определяют его **сложность**. Например, если не ограничивать рост дерева (большое значение max_depth или маленькие min_samples_split/min_samples_leaf), дерево может вырасти очень глубоким и подстроиться под каждую точку обучающей выборки - это приводит к **переобучению** (отличное качество на обучении, но плохое на новых данных, потому что дерево выучило шум). Напротив, если задать слишком жёсткие ограничения - например, max_depth = 2 уровня или требовать минимум 50 образцов на лист - дерево будет очень неглубоким, оно может не уловить важных зависимостей и будет **недообученным** (oversimplified моделью). Обычно параметры подбирают так, чтобы контролировать баланс: например, уменьшая max_depth или увеличивая min_samples_leaf, мы **снижаем риск переобучиться**, заставляя дерево быть более обобщающим. Но если перестараться, можем упустить важные разделения - тогда появится недообучение. Таким образом, гиперпараметры дерева (глубина, минимальный размер узла и т.д.) действуют как "регуляторы сложности": чем более свободно рост дерева, тем выше риск переобучения; чем сильнее ограничения, тем выше шанс недообучения.
