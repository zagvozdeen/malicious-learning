---
question: Ключевые шаги модели логистической регрессии. Обоснование функции потерь
module: Теория
---

Логистическая регрессия – это модель для классификации, и её основные шаги такие:
1. **Форма модели (сигмоидная функция):** вместо непосредственного предсказания класса модель вычисляет вероятность принадлежности к классу. Для этого берётся линейная комбинация признаков z = w0 + w1*x1 + ... + wn*xn, а затем она преобразуется сигмоидой: p = 1/(1 + exp(-z)). Результат p – это вероятность положительного класса (в диапазоне 0 до 1).
1. **Функция потерь (логистическая):** для обучения модель максимизирует правдоподобие или эквивалентно минимизирует логистическую (кросс-энтропийную) потерю. В упрощённом виде на один образец: если y = 1, штраф = -ln(p), если y = 0, штраф = -ln(1 - p). Эта функция выбрана, потому что она естественно возникает из максимума правдоподобия: предполагая, что данные метки 0/1 генерируются с вероятностями p и 1-p, логарифмическая функция потерь наказывает модель, когда она уверенно ошибается (даёт p близкое к 1 при y=0 или наоборот). Это заставляет веса смещать модель, чтобы классы были лучше разделены по вероятности.
1. **Обучение модели:** явного формульного решения, как в линейной регрессии, здесь нет, поэтому оптимальные веса ищутся итеративно. Обычно применяется градиентный спуск или его варианты (стохастический, BFGS и т.д.), чтобы найти w, минимизирующие суммарную логистическую потерю на обучении. В процессе градиент рассчитывается на основе того, насколько прогноз p отклоняется от фактического y для каждого примера.

Таким образом, логистическая регрессия строит разделяющую гиперплоскость между классами, но вместо минимизации MSE она минимизирует логарифмическую функцию потерь. Эта функция потерь обоснована статистически (через максимизацию правдоподобия для Bernoulli-распределения ответов) и практически эффективна: она дифференцируема и фокусирует обучение на тех примерах, где модель сильно ошибается.
