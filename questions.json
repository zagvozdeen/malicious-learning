[
  {
    "id": 1,
    "module":"Теория",
    "question": "Почему рекомендуется выполнять проверку модели на валидационных данных (в качестве альтернативы простому использованию всех тренировочных данных)?",
    "answer": "При обучении модели важно проверять её качество на данных, которых она не видела во время обучения. Если оценивать качество только на тех же обучающих данных, модель может показать отличные результаты, но это лишь потому, что она <strong>выучила тренировочные примеры наизусть (переобучилась)</strong>. Валидационная выборка имитирует новые данные: на ней мы проверяем, насколько хорошо модель обобщает и справляется с незнакомыми примерами. Таким образом, проверка на валидации позволяет <strong>оценить модель на новых данных и вовремя заметить переобучение</strong>, а также подобрать оптимальные гиперпараметры, не заглядывая в тестовые данные."
  },
  {
    "id": 2,
    "module": "Теория",
    "question": "Проверка модели на каких данных дает представление о недообучении, а какие для оценки переобучения?",
    "answer": "Чтобы понять, недообучена ли модель, смотрят на качество на <strong>обучающих данных</strong>. Если даже на тренировочной выборке модель показывает большие ошибки, значит она слишком простая или плохо настроена (недообучена). А вот для выявления <strong>переобучения</strong> сравнивают качество на обучении и на <strong>валидационных/тестовых данных</strong>. Если на обучении всё отлично (низкая ошибка), а на новых данных значительно хуже, это признак переобучения: модель подогналась под тренировку и плохо обобщает."
  },
  {
    "id": 3,
    "module": "Теория",
    "question": "Почему при предварительной обработке числовых и категориальных признаков необходимо сохранять параметры преобразования, а не преобразовывать тестовые (новые) данные отдельно?",
    "answer": "При обработке признаков (например, масштабировании чисел или кодировании категорий) нужно применять <strong>точно такие же преобразования</strong> к новым данным, что и к обучающим. Мы сначала вычисляем параметры на тренировочной выборке: например, среднее и стандартное отклонение для нормализации или какие категориальные значения соответствуют каким кодам. Эти параметры обязательно сохраняют и используют затем для тестовых данных. Если же обработать новые данные \"с нуля\" по своим параметрам, то модель получит несопоставимые значения. Проще говоря, несохранение параметров приведёт к тому, что модель увидит данные в другом масштабе или с другими кодами, и её прогнозы станут некорректными. Кроме того, пересчитывая параметры по тестовым данным, мы бы подсмотрели на тест (утечка данных), чего делать нельзя."
  },
  {
    "id": 4,
    "module": "Теория",
    "question": "Как можно использовать категориальные данные в моделях машинного обучения? В каких случаях какой подход будет предпочтительней?",
    "answer": "Категориальные признаки нельзя напрямую подать в большинство моделей, их нужно сначала перевести в численное представление. Основные подходы:<br><ul><br>  <li><strong>Порядковое кодирование (Label Encoding):</strong> каждому категориальному значению присваивается числовой код. Применимо, когда категории имеют естественный порядок (например, «низкий», «средний», «высокий»).</li><br>  <li><strong>One-Hot Encoding (бинарное кодирование):</strong> создаются новые бинарные столбцы по числу категорий, где значение 1 отмечает присутствие категории. Предпочтительно для номинальных (неупорядоченных) категорий с относительно небольшим числом уникальных значений. Например, признак «цвет» можно закодировать столбцами \"цвет_красный\", \"цвет_синий\", ... Значения не имеют порядка, поэтому one-hot подходит.</li><br>  <li><strong>Целевое кодирование (Target Encoding):</strong> категория заменяется на некоторое числовое значение, вычисленное из обучающих данных (например, среднее значение целевой переменной для каждой категории). Используется осторожно, чтобы не допустить утечку информации. Предпочтительно при очень многих уникальных категориях, когда one-hot создавать слишком громоздко.</li><br>  <li><strong>Hash Encoding (хеширование категорий):</strong> категории пропускаются через хеш-функцию для получения фиксированного числа столбцов. Это уменьшает размерность и избегает взрыва признаков при большом количестве категорий, но может приводить к коллизиям (разным категориям в один код).</li><br>  <li><strong>Специализированные алгоритмы:</strong> Некоторые алгоритмы умеют работать с категориями напрямую. Например, CatBoost назван так, потому что эффективно обрабатывает категориальные признаки внутри себя. В таких случаях можно подавать категориальные поля без ручного кодирования: модель сама преобразует их (обычно внутренне через схему подобную целевому кодированию с контролем).</li><br></ul>Выбор подхода зависит от данных: если категорий мало, удобен one-hot (он прост и не вносит предположений). Если категориальных значений очень много, лучше применять методы вроде target encoding или хеширования, чтобы не раздувать пространство признаков. При наличии порядка в значениях используют порядковое числовое кодирование. А если используете модели, умеющие работать с категориями (например CatBoost), можно довериться им."
  },
  {
    "id": 5,
    "module": "Теория",
    "question": "Ключевые шаги модели линейной регрессии. Обоснование функции потерь",
    "answer": "Модель линейной регрессии включает следующие ключевые шаги:<br><ol><br>  <li><strong>Задаём линейную форму модели:</strong> предполагается, что целевая переменная связана с признаками линейно: y_pred = w0 + w1*x1 + ... + wn*xn. То есть, мы ищем коэффициенты w, при которых прямая (или гиперплоскость) наилучшим образом приближает исходные данные.</li><br>  <li><strong>Выбираем функцию потерь:</strong> обычно используется <strong>среднеквадратичная ошибка (MSE)</strong>, то есть средний квадрат разницы между предсказанием и истинным значением. Эта функция потерь удобно дифференцируется и даёт больший штраф за сильные отклонения, что способствует поиску \"наилучшей\" по среднему квадратическому критерию прямой. Кроме того, MSE соответствует предположению о нормальном (гауссовском) распределении ошибок: выбор MSE эквивалентен методу максимального правдоподобия при таких ошибках.</li><br>  <li><strong>Оптимизируем параметры (обучение):</strong> цель – найти коэффициенты w, минимизирующие функцию потерь на обучающих данных. В случае линейной регрессии задача сведения к решению нормальных уравнений (X<sup>T</sup>Xw = X<sup>T</sup>y) или решается численно методом градиентного спуска. В итоге получаем набор весов w, при котором прямая наилучшим образом приближает тренировочные точки.</li><br></ol><br>Почему берут именно MSE? Эта функция потерь обеспечивает единственное минимальное решение (выпуклая парабола по параметрам) и математически удобна – её производная по каждому параметру проста, что позволяет легко найти оптимальные w. К тому же, как упомянуто, она имеет статистическое обоснование (минимизация MSE равносильна поиску МНК-оценки при нормальных шумах)."
  },
  {
    "id": 6,
    "module": "Теория",
    "question": "Ключевые шаги модели логистической регрессии. Обоснование функции потерь",
    "answer": "Логистическая регрессия – это модель для классификации, и её основные шаги такие:<br><ol><li><strong>Форма модели (сигмоидная функция):</strong> вместо непосредственного предсказания класса модель вычисляет вероятность принадлежности к классу. Для этого берётся линейная комбинация признаков z = w0 + w1*x1 + ... + wn*xn, а затем она преобразуется сигмоидой: p = 1/(1 + exp(-z)). Результат p – это вероятность положительного класса (в диапазоне 0 до 1).</li><li><strong>Функция потерь (логистическая):</strong> для обучения модель максимизирует правдоподобие или эквивалентно минимизирует логистическую (кросс-энтропийную) потерю. В упрощённом виде на один образец: если y = 1, штраф = -ln(p), если y = 0, штраф = -ln(1 - p). Эта функция выбрана, потому что она естественно возникает из максимума правдоподобия: предполагая, что данные метки 0/1 генерируются с вероятностями p и 1-p, логарифмическая функция потерь наказывает модель, когда она уверенно ошибается (даёт p близкое к 1 при y=0 или наоборот). Это заставляет веса смещать модель, чтобы классы были лучше разделены по вероятности.</li><li><strong>Обучение модели:</strong> явного формульного решения, как в линейной регрессии, здесь нет, поэтому оптимальные веса ищутся итеративно. Обычно применяется градиентный спуск или его варианты (стохастический, BFGS и т.д.), чтобы найти w, минимизирующие суммарную логистическую потерю на обучении. В процессе градиент рассчитывается на основе того, насколько прогноз p отклоняется от фактического y для каждого примера.</li><br></ol>Таким образом, логистическая регрессия строит разделяющую гиперплоскость между классами, но вместо минимизации MSE она минимизирует логарифмическую функцию потерь. Эта функция потерь обоснована статистически (через максимизацию правдоподобия для Bernoulli-распределения ответов) и практически эффективна: она дифференцируема и фокусирует обучение на тех примерах, где модель сильно ошибается."
  },
  {
    "id": 7,
    "module": "Теория",
    "question": "Почему использование L1 регуляризации для линейных моделей может привести к отбору признаков (в отличие от L2 регуляризации)?",
    "answer": "L1-регуляризация (например, Lasso) добавляет к функции ошибки сумму модулей коэффициентов (|w|). Особенность L1 в том, что она может занулить некоторые веса полностью. Оптимизируя с таким штрафом, модель стремится обнулить наименее значимые признаки, чтобы снизить суммарный модуль весов. Геометрически ограничения L1 образуют \"ромб\" в пространстве параметров, у которого вершины на осях – поэтому оптимум часто лежит на одной из осей, где какой-то вес равен нулю. В итоге L1 дает <strong>разреженное решение, многие коэффициенты становятся точно 0</strong>. Это фактически выполняет отбор признаков: признаки с нулевыми весами исключаются из модели.<br>L2-регуляризация (ридж-регрессия) напротив использует сумму квадратов весов. Она стремится только \"сжать\" веса к нулю, но не делает их ровно нулевыми (оптимум при L2 – гладко смещён к нулю без резких углов). Поэтому L2 уменьшает влияние слабых признаков, но редко обнуляет их полностью. Именно поэтому L1 может отобрать небольшой поднабор важных признаков, а L2 скорее будет удерживать все понемногу."
  },
  {
    "id": 8,
    "module": "Теория",
    "question": "Что является степенью значимости признаков при использовании метода главных компонент?",
    "answer": "Метод главных компонент (PCA) преобразует исходные признаки в новые – главные компоненты. Каждая такая компонента получается из собственных векторов ковариационной матрицы и имеет свою собственную величину дисперсии (собственное значение). Вот эта дисперсия и служит мерой значимости компоненты: <strong>чем больше дисперсия (собственное значение) главной компоненты, тем больше вариации данных она объясняет</strong>, и тем более важной считается эта компонента. Обычно компоненты сортируют по убыванию дисперсии. Например, первая главная компонента имеет наибольшую дисперсию и захватывает максимальную долю информации (разброса) исходных данных. Соответственно, доля объясненной дисперсии каждой компонентой показывает её значимость по сравнению с другими."
  },
  {
    "id": 9,
    "module": "Теория",
    "question": "Перечислите хотя бы 5 функций, которые можно использовать для оценки близости векторов (https://arxiv.org/pdf/1708.04321.pdf - ребята тут чуть ли не 100 метрик сравнивают)",
    "answer": "Сходство или расстояние между векторами можно измерять разными метриками. Вот несколько популярных вариантов:<br><ul><br>  <li><strong>Евклидово расстояние:</strong> обычная \"прямолинейная\" дистанция между точками в пространстве. Вычисляется как квадратный корень из суммы квадратов разностей по каждой координате. Чем оно меньше, тем ближе (похожи) векторы.</li><br>  <li><strong>Манхэттенское расстояние (L1):</strong> сумма модулей разностей координат. Это расстояние по осям (как будто идёте по сетке улиц Манхэттена). Тоже чем меньше, тем векторы ближе.</li><br>  <li><strong>Косинусное сходство:</strong> измеряет угол между векторами. Вычисляется как косинус угла между ними = (A·B)/(|A|*|B|). Если косинус близок к 1, векторы очень похожи (почти сонаправлены); близок к 0 – они ортогональны (нет сходства); если -1 – противоположны.</li><br>  <li><strong>Коэффициент Пирсона (корреляция):</strong> хотя это мера линейной зависимости, его можно использовать как меру схожести профилей двух векторов. Корреляция близкая к 1 означает, что векторы изменяются (повышаются/понижаются) синхронно (после нормализации среднего и масштаба).</li><br>  <li><strong>Расстояние Чебышёва:</strong> максимальная по модулю разность по одной из координат. То есть берём координатную разницу, и расстояние равно наибольшему из этих отклонений. Полезно, когда важно наибольшее различие.</li><br>  <li><strong>Расстояние Хэмминга:</strong> число позиций, в которых два вектора (часто бинарных или строк одинаковой длины) отличаются. Применяется, например, для сравнения последовательностей из 0/1 или текстовых строк фиксированной длины.</li><br>  <!-- We listed 6 metrics for comprehensiveness --><br></ul><br>Это лишь некоторые функции: существует множество и других (Джаккарда для бинарных признаков, махаланобисово расстояние с учётом корреляций и т.д.). В зависимости от задачи и природы данных выбирают подходящую метрику близости."
  },
  {
    "id": 10,
    "module": "Теория",
    "question": "В чем основное отличие использования метода k-ближайших соседей в качестве классификатора и регрессии?",
    "answer": "Метод k-ближайших соседей (k-NN) работает схожим образом и для классификации, и для регрессии, но способ получения ответа отличается:<br><ul><br>  <li><strong>k-NN как классификатор:</strong> смотрит на k ближайших точек и определяет класс нового объекта по <strong>голосованию</strong> этих соседей. Проще говоря, выбирается тот класс, который наиболее часто встречается среди k соседей (возможен также взвешенный вариант, где ближние соседи сильнее влияют).</li><br>  <li><strong>k-NN как регрессор:</strong> берёт k ближайших точек и возвращает как предсказание <strong>среднее (например, арифметическое)</strong> их числовых значений целевой переменной. То есть просто усредняет значения цели соседей (иногда тоже с весом по расстоянию: ближним даётся больший вес).</li><br></ul><br>Итого: при классификации k-NN выдаёт категорию на основе большинства соседей, а при регрессии – число, вычисленное на основе значений соседей."
  },
  {
    "id": 11,
    "module": "Теория",
    "question": "Как можно ускорить применение методов ближайших соседей?",
    "answer": "Алгоритмы ближайших соседей могут быть медленными при большом числе точек, так как на каждый запрос приходится вычислять расстояние до всех образцов. Для ускорения применяют:<br><ul><br>  <li><strong>Специальные структуры данных:</strong> предварительно организовать обучающие точки в структуру, позволяющую быстрее искать ближайших. Например, k-d дерево, ball-tree, cover-tree и им подобные. Эти структуры разбивают пространство и позволяют отбрасывать сразу целые группы точек при поиске ближайших, вместо перебора всех.</li><br>  <li><strong>Аппроксимация (Approximate NN):</strong> использовать алгоритмы приближённого поиска ближайших соседей. Они не гарантируют нахождение абсолютно самого близкого соседа, но сильно ускоряют поиск. Примеры: локально-чувствительное хеширование (LSH), или библиотеки типа FAISS, Annoy, которые осуществляют быстрый поиск с небольшой погрешностью.</li><br>  <li><strong>Сокращение размерности:</strong> если признаков очень много, методы вроде PCA или t-SNE (для последующей кластеризации) могут снизить размерность, чтобы поиск соседей происходил в меньшем пространстве (и, возможно, быстрее).</li><br>  <li><strong>Простое кеширование и параллелизация:</strong> если нужно выполнять много поисков соседей, результаты можно частично кешировать; а вычисления расстояний до разных точек выполняются параллельно (много потоков или GPU), что ускоряет суммарно.</li><br></ul><br>Основная идея: избежать полного пересчёта расстояний до всех точек. Структуры данных позволяют искать за время намного меньше, чем линейное по количеству объектов, а приближенные методы делают разумный компромисс между скоростью и точностью."
  },
  {
    "id": 12,
    "module": "Теория",
    "question": "Какие особенности применение градиентного спуска в методе визуализации t-SNE?",
    "answer": "Алгоритм t-SNE настраивает расположение точек в низкоразмерном пространстве с помощью оптимизации специальной функции стоимости. Тут используются итерации градиентного спуска, и у этого процесса есть несколько особенностей:<br><ul><br>  <li><strong>Нестандартная целевая функция:</strong> t-SNE минимизирует расхождение (дивергенцию Кульбака-Лейблера) между распределениями расстояний в исходном и целевом пространстве. Эта функция сложная и <strong>не имеет единственного минимума</strong> (не выпуклая), поэтому градиентный спуск может застревать в локальных минимальных конфигурациях в зависимости от начальной инициализации.</li><br>  <li><strong>Высокая вычислительная сложность:</strong> прямой градиентный спуск требует учитывать пары всех точек (O(n<sup>2</sup>) операций) для расчёта сил притяжения/отталкивания между каждой парой. На практике для ускорения используют приближённые методы (например, алгоритм Barnes-Hut для аппроксимации дальних взаимодействий), чтобы сделать вычисления быстрее.</li><br>  <li><strong>Специальные приёмы в оптимизации:</strong> чтобы добиться хорошего результата, t-SNE применяет техники, такие как <em>early exaggeration</em> (искусственно увеличивает притяжение кластеров на первых итерациях, чтобы раздвинуть группы точек), и использует momentum (инерцию) при обновлении позиций, чтобы обходить неглубокие ловушки локальных минимумов. Шаг градиентного спуска (learning rate) тоже нужно подбирать: слишком большой – точки могут \"разлететься\", слишком маленький – процесс займет много времени или застрянет.</li><br></ul><br>Таким образом, градиентный спуск в t-SNE – итеративная процедура, требующая аккуратной настройки. Это не простой спуск как в линейной регрессии: он оптимизирует нетривиальную функцию, занимает больше времени и зависит от параметров (перплексии, начальных условий) для получения удачной визуализации."
  },
  {
    "id": 13,
    "module": "Теория",
    "question": "Что означает буква t в методе визуализации t-SNE?",
    "answer": "Буква \"t\" в названии t-SNE обозначает <strong>t-распределение (Student t-distribution)</strong>. Алгоритм называется \"t-distributed Stochastic Neighbor Embedding\", потому что при проекции данных в низкое пространство он использует распределение Стьюдента (с одной степенью свободы) для моделирования расстояний между точками. Такое распределение имеет тяжёлые хвосты и позволяет точкам не слипаться в центре, лучше разделяя кластеры на визуализации."
  },
  {
    "id": 14,
    "module": "Теория",
    "question": "В чем ключевая идея метода визуализации t-SNE?",
    "answer": "Ключевая идея t-SNE – расположить объекты в низкоразмерном (например, 2D) пространстве так, чтобы сохранить структуру их близости из исходных данных. Алгоритм стремится <strong>оставить похожие точки близко друг к другу, а непохожие – далеко друг от друга</strong> на итоговом графике. Для этого он переводит расстояния между точками в исходном пространстве в вероятности \"быть соседями\" и пытается подобрать такие координаты в 2D, чтобы аналогичные вероятности (но с t-распределением) получились между точками на плоскости. В результате локальная структура данных (близкие группы, кластеры) отображается верно: точки одного кластера сбиваются вместе, а разные кластеры оказываются разделены."
  },
  {
    "id": 15,
    "module": "Теория",
    "question": "В чем отличие построение линий в линейной / логистической регрессии и в методе опорных векторов?",
    "answer": "Под \"построением линий\" понимают нахождение разделяющей границы (линии или гиперплоскости) между классами или приближающей зависимости. Отличия между методами:<br><ul><br>  <li><strong>Линейная регрессия:</strong> здесь «линия» – это линия на графике, приближающая непрерывные значения. Она строится так, чтобы минимизировать среднеквадратичную ошибку. В контексте классификации линейную регрессию обычно не применяют (она не выдает класс, только число), поэтому прямая тут – не граница классов, а линия прогноза чисел.</li><br>  <li><strong>Логистическая регрессия:</strong> строит разделяющую линию (гиперплоскость) между классами, но делает это через вероятностную модель. Граница проходит там, где модель дает вероятность 0.5. Оптимизация идет по логистической функции потерь: учитываются все объекты, и веса подбираются так, чтобы максимум объектов оказались правильно классифицированы с учётом их удаленности от границы (ошибка постепенно возрастает, чем дальше объект на неправильной стороне).</li><br>  <li><strong>SVM (Метод опорных векторов):</strong> также находит разделяющую гиперплоскость, но критерий отличается: SVM максимизирует зазор (margin) между классами. Граница проводится так, чтобы расстояние от неё до ближайших точек каждого класса было максимально возможным. При этом положение этой линии определяется лишь несколькими близкими к ней точками, которые называются опорными векторами. Остальные примеры, лежащие далеко от границы, на решение не влияют напрямую. Если класс неразделим точной линией, SVM допускает ошибки (служебные переменные) или использует ядра для нелинейного разделения, но всё равно стремится к максимальному зазору.</li><br></ul><br>Таким образом, logistic regression и SVM оба строят линейную границу для классов, но логистическая регрессия оптимизирует вероятностную (логарифмическую) функцию ошибки, учитывая все точки, а SVM фокусируется на максимизации отступа, ориентируясь на \"крайние\" точки. Линейная же регрессия решает совсем другую задачу – приближает числа, а не разделяет категории."
  },
  {
    "id": 16,
    "module": "Теория",
    "question": "Почему Kernel trick помогает улучшить результаты метода опорных векторов?",
    "answer": "Ядровой трюк (kernel trick) позволяет SVM эффективно строить нелинейные разделяющие поверхности. Без ядра SVM может проводить только прямую (гиперплоскость) в исходном пространстве признаков. Однако с ядром происходит следующее: исходные данные неявно отображаются в пространство высокой (вплоть до бесконечности) размерности, где их можно разделить линейно. Kernel trick вычисляет скалярные произведения между образами точек в этом новом пространстве без явного вычисления координат – через специальную функцию подобия (ядро). Например, радиально-базисное ядро (RBF) по сути учитывает расстояние между точками и позволяет отделять образцы по гауссовым \"пузырям\". В результате SVM находит линейную границу в расширенном пространстве, которая отображается как нелинейная сложная граница в исходном пространстве признаков. Благодаря этому SVM может решать задачи, где данные разделяются не прямой, а более хитрой кривой – то есть повышается качество на сложных датасетах. И все это достигается без существенного увеличения вычислительной сложности (благодаря тому, что вычисления идут через ядро, а не явное построение огромного количества признаков)."
  },
  {
    "id": 17,
    "module": "Теория",
    "question": "В чем отличие радиальной базисной функции от меры подобия, используемой в t-SNE для оценки близости в исходном пространстве признаков?",
    "answer": "Радиальная базисная функция (RBF), как ядро или мера сходства, обычно имеет фиксированный параметр ширины σ для всех пар точек: сходство K(x,y) = exp(-||x - y||<sup>2</sup> / (2σ<sup>2</sup)). В t-SNE же схожесть точек в исходном пространстве определяется через условные вероятности: для каждой точки i подбирается такая σ<sub>i</sub>, что распределение вероятностей соседей имеет заданную перплексию. То есть мера подобия в t-SNE адаптивна: <strong>у каждой точки свой масштаб σ</strong>, зависящий от плотности её окружения. При вычислении p<sub>j|i</sub> = exp(-||x_i - x_j||<sup>2</sup>/(2σ<sub>i</sub><sup>2</sup>)) результаты нормируются по всем j для фиксированного i. Таким образом, отличие: RBF-ядро глобально одинаково для всех (симметричная мера сходства с одной настроенной σ), а в t-SNE степень \"близости\" настроена локально для каждой точки (чтобы каждая точка имела примерно одинаковое число близких соседей согласно перплексии). В итоге t-SNE уделяет равное внимание локальным структурам в разных частях пространства, тогда как RBF с фиксированной σ может переоценить плотные скопления или недооценить разрежённые области."
  },
  {
    "id": 18,
    "module": "Теория",
    "question": "В чем разница между использованием деревьев решений в задачах классификации и регрессии?",
    "answer": "Деревья решений могут применяться и для классификации, и для регрессии, но их поведение отличается:<br><ul><br>  <li><strong>Дерево классификации:</strong> на каждом узле выбирается признак и порог (или категория), которые лучше всего разделяют данные по классам. Критерий качества разделения – снижение перемешанности классов (например, прирост информации или уменьшение impurity: энтропии или Gini). Листья дерева содержат определенный класс (или распределение вероятностей по классам, но конечный прогноз обычно самый частый класс в листе). То есть, проходя по условиям, в итоге мы определяем, к какому классу относится объект.</li><br>  <li><strong>Дерево регрессии:</strong> здесь узел делится так, чтобы минимизировать разброс численных значений целевой переменной в дочерних узлах. Часто используют критерий минимизации суммы квадратов ошибок (MSE) или дисперсии. В листе вместо класса хранится некоторое числовое значение – обычно среднее всех ответов обучающих объектов, попавших в этот лист. Предсказание для нового объекта – это среднее значение в листе, куда он попадает. Иными словами, дерево пытается разделять данные на области, внутри которых целевая переменная примерно постоянна, и предсказывает эту постоянную.</li><br></ul><br>В итоге: различие в критериях обучения (классификация – стремится к чистоте по классам, регрессия – к гомогенности численных значений) и в типе предсказания (класс vs число)."
  },
  {
    "id": 19,
    "module": "Теория",
    "question": "Представьте, что вы обучили модель Случайный Лес, в котором 50 деревьев. Рассмотрим 2 случая: 1) Мы убираем первое дерево из модели 2) Мы убираем последнее дерево из модели. Предположите, какая модель будет лучше (или они +- одинаковы) и почему?",
    "answer": "В случайном лесе (Random Forest) все деревья обучаются независимо и примерно одинаково влияют на итог. Нет понятия последовательности как в бустинге, поэтому удаление \"первого\" или \"последнего\" дерева по сути одно и то же. В обоих случаях останется ансамбль из 49 деревьев. Разницы между тем, какое именно дерево вы убрали, практически не будет: модель 1) и модель 2) будут показывать <strong>примерно одинаковое качество</strong>. Каждое отдельное дерево вносит небольшой вклад, а порядок их добавления не играет роли (их же обучали параллельно). Возможно, мизерное отличие может возникнуть, если какое-то удаленное дерево было чуть сильнее или слабее среднего, но систематически \"первое\" или \"последнее\" ничем не отличается."
  },
  {
    "id": 20,
    "module": "Теория",
    "question": "Представьте, что вы обучили модель с использованием Градиентного Бустинга, в котором 50 деревьев. Рассмотрим 2 случая: 1) Мы убираем первое дерево из модели 2) Мы убираем последнее дерево из модели. Предположите, какая модель будет лучше (или они +- одинаковы) и почему?",
    "answer": "В градиентном бустинге деревья строятся последовательно: каждое новое дерево пытается исправить ошибки всех предыдущих. Поэтому порядок здесь очень важен. Если удалить <strong>первое дерево</strong>, то оставшиеся 49 деревьев уже не будут корректно работать, ведь они обучались на остатках с учётом существования того первого дерева. По сути, убрав базовое дерево, мы теряем основу ансамбля, и последующие деревья окажутся как бы \"настроены не на ту базу\". Это сильно собьёт качество. Если же удалить <strong>последнее дерево</strong>, первые 49 всё равно сформируют почти готовую модель, просто не получив последней коррекции. Последнее дерево обычно вносит небольшое улучшение. Поэтому модель без последнего дерева будет немного проще и чуть менее точной, чем полная, но её качество, скорее всего, выше, чем у варианта без первого дерева. Иными словами, отсутствие самого первого дерева гораздо критичнее для итоговой точности, чем отсутствие последнего."
  },
  {
    "id": 21,
    "module": "Теория",
    "question": "Рассмотрим две ситуации: 1) Вы обучили две модели классификации Случайный Лес, по 25 деревьев в каждой. Ваше итоговое предсказание - усреднение двух моделей 2) Вы обучили одну модель классификации Случайный Лес, в которой 50 деревьев. Остальные гиперпараметры моделей совпадают. Предположите, какая модель будет лучше (или они +- одинаковы) и почему?",
    "answer": "В обоих случаях по сути используются 50 деревьев, просто в первом случае они разделены на два ансамбля по 25 с последующим усреднением их результатов, а во втором – объединены в один ансамбль из 50 деревьев. Если все гиперпараметры и обучающие данные те же, то ожидается <strong>примерно одинаковое качество</strong> у обоих подходов. Причина в том, что усреднение двух независимых случайных лесов эквивалентно увеличению числа деревьев: вы всё равно берёте мнения 50 случайных деревьев в сумме. Случайный лес уже строит деревья случайно, и объединение их – центральная идея. Возможно, совсем незначительная разница может быть вызвана случайностью: два отдельных леса могут породить чуть более разнообразные модели, но если они обучались на одной выборке и по тем же параметрам, то большой разницы не будет. В целом 50 деревьев дают примерно ту же мощность, как ни группируй их."
  },
  {
    "id": 22,
    "module": "Теория",
    "question": "Есть градиентный бустинг от яндекса. Он называется \"CatBoost\". Михаил Сергеевич из Санкт-Петербурга спрашивает: \"Ну почему?\" (что означает приставка Cat и почему кошки тут не причем {почти})",
    "answer": "Название <strong>CatBoost</strong> расшифровывается как \"Categorical Boosting\" – то есть бустинг, хорошо работающий с категориальными признаками. Приставка \"Cat\" означает именно <em>категории</em>, а вовсе не кошек: алгоритм разработан с упором на эффективное кодирование и использование категориальных (категоричных) данных. Конечно, совпадение с английским словом \"cat\" (кот) – это удачная игра слов, но непосредственно с пушистыми котиками алгоритм не связан. Разве что в логотипе можно увидеть кота, но <strong>главное</strong> – это работа с категориями."
  },
  {
    "id": 23,
    "module": "Теория",
    "question": "Сравните хотя бы 2 метода градиентного бустинга (как с т.з. «внутренностей», так и с точки зрения применения на практике)",
    "answer": "Сравним популярные реализации градиентного бустинга: например, <strong>XGBoost</strong>, <strong>LightGBM</strong> и <strong>CatBoost</strong>:<br><ul><br>  <li><strong>XGBoost (Extreme Gradient Boosting):</strong> использует построчный (level-wise) рост деревьев, тщательно просматривая возможные разбиения. Имеет продвинутые возможности регуляризации (L1, L2 на листьях), встроенную обработку пропусков. XGBoost вычисляет точные решения или по желанию использует гистограммы. На практике XGBoost хорошо работает \"из коробки\", хотя может быть медленнее, чем некоторые новые реализации, особенно на очень больших данных. Он стабилен и даёт предсказуемое качество, требует явного кодирования категорий (сам их не обрабатывает).</li><br>  <li><strong>LightGBM:</strong> акцент на скорость и память. Вместо перебора всех порогов признаков, LightGBM использует гистограммный подход (признаки распределяются по бинам). Кроме того, он растит деревья \"по листьям\" (leaf-wise): в каждой итерации расширяет ту ветвь, где выигрыш по метрике максимален, что может приводить к более глубоким деревьям. Это очень ускоряет обучение на больших наборах данных. LightGBM часто быстрее XGBoost при сопоставимом качестве. Однако из-за агрессивного роста по листьям есть риск переобучиться, поэтому важны ограничивающие параметры (макс. глубина или мин. размер листа). Категориальные признаки он умеет обрабатывать через специальный способ (сортировка по частоте и поиск разбиения), но часто применяют и классическое one-hot при необходимости.</li><br>  <li><strong>CatBoost:</strong> ключевая фишка – встроенная обработка категориальных признаков с помощью методик, избегающих утечки (например, кодирование категориальных значений по целевому признаку с использованием случайных перестановок (чтобы избежать утечки информации)). Он строит так называемые симметричные деревья (каждый уровень сбалансирован по признакам), что облегчает использование на GPU и стабилизирует структуру модели. По скорости CatBoost обычно сравним с LightGBM (иногда медленнее на числовых данных из-за доп. расчётов для категорий). Зато на данных с множеством категорий он, как правило, показывает превосходное качество без ручной подготовки. В применении CatBoost удобен тем, что требует минимум преобразований данных и довольно устойчив к настройкам (меньше параметров надо крутить для начала).</li><br></ul><br>На практике выбор между ними часто зависит от данных и условий: XGBoost – надёжный выбор, особенно если нужны регуляризация и проверенный временем алгоритм. LightGBM – когда важны скорость и ресурсная эффективность, особенно на больших выборках. CatBoost – когда много категориальных признаков или хочется получить высокий результат без долгого подбора кодирования для категорий. По качеству все три, как правило, дают близкие результаты, но в конкретных задачах один может сработать лучше других. Кроме этих, есть и другие бустинги (например, оригинальный AdaBoost, HistGradientBoosting в scikit-learn), но XGBoost/LightGBM/CatBoost – наиболее популярны сегодня."
  },
  {
    "id": 24,
    "module": "Теория",
    "question": "Предположите, почему ансамблевые модели, основанные на деревьях решений, работают лучше на табличных данных чем Нейронные сети (можно вдохновиться статьёй https://arxiv.org/pdf/2207.08815.pdf)",
    "answer": "На табличных (структурированных) данных алгоритмы ансамблей деревьев (случайные леса, бустинг) часто превосходят нейросети. Возможные причины:<br><ul><br>  <li><strong>Учет структуры признаков:</strong> Деревья решений естественным образом обрабатывают различные типы признаков: числовые, категориальные (могут разделять по категории без кодирования), могут игнорировать неинформативные признаки. Нейросети же требуют тщательной нормализации, кодирования категорий и достаточного количества данных, чтобы самостоятельно выучить полезные представления.</li><br>  <li><strong>Меньший риск переобучения на малых данных:</strong> Табличные датасеты часто относительно невелики (сотни или тысячи образцов), и глубокая нейросеть легко переобучится на таком объёме. Ансамбли деревьев, особенно с регуляризацией (как в бустинге) или усреднением (как в лесу), более устойчивы на небольших данных, они встроенно выполняют байесовское усреднение моделей или штрафуют сложность.</li><br>  <li><strong>Хватка нелинейных зависимостей и взаимодействий:</strong> Деревья делят пространство признаков на регионы, автоматически учитывая нелинейности и комбинации признаков (каждое разбиение может рассматриваться как взаимодействие условий по разным признакам). Нейросети тоже способны на нелинейности, но им нужно гораздо больше данных и времени, чтобы выучить аналогичные зависимости. На табличных данных часто важны чёткие правила или пороги – деревья их ловят буквально, а нейросеть может обобщать слишком сглаженно или неправильно без достаточного обучения.</li><br>  <li><strong>Простота настройки и знания:</strong> Ансамблевых моделей с деревьями обычно легче добиться хорошего качества \"из коробки\". Дефолтные параметры бустинга или случайного леса часто уже дают приличный результат. Нейросеть же требует выбора архитектуры, настройки множества гиперпараметров (слои, нейроны, скорость обучения и т.д.) и возможно много экспериментов. Без глубокой экспертизы нейросеть может уступать более простым в настройке алгоритмам.</li><br></ul><br>В целом, деревья хорошо приспособлены к разнородным табличным данным, учитывают важность признаков (можно легко получать важности), не требуют большого объёма данных, а их ансамбли уменьшают переобучение. Нейронные сети раскрывают потенциал на очень больших и сложных данных (изображения, текст), а на типичных табличных задачах часто проигрывают правильно настроенному бустингу."
  },
  {
    "id": 25,
    "module": "Теория",
    "question": "Как можно отбирать значимые признаки в задачах обработки данных методами машинного обучения?",
    "answer": "Существует множество подходов к отбору (выбору) наиболее важных признаков в датасете:<br><ul><br>  <li><strong>Filter-методы (фильтрационные):</strong> признаки отбираются на основе статистических свойств, без участия конкретного алгоритма. Например, можно вычислить корреляцию признака с целевой переменной и отбросить те, у которых связь слабая. Для классификации часто применяют тесты χ<sup>2</sup> или оценку информации (information gain) между признаком и классом. Также простой фильтр – отсеять признаки с почти нулевой вариативностью (константные или почти константные).</li><br>  <li><strong>Wrapper-методы (обёртки):</strong> тут используется модель (алгоритм) как \"оценщик\" качества подмножества признаков. Пробуются различные комбинации признаков и выбирается комбинация, дающая наилучшую метрику модели (обычно по кросс-валидации). Примеры: рекурсивный отбор признаков (RFE), пошаговый отбор (добавляем по одному признаку, оставляя те, что улучшают модель) или наоборот, по одному убираем и смотрим, ухудшилось ли качество.</li><br>  <li><strong>Embedded-методы (встроенные):</strong> алгоритм обучения сам делает отбор признаков в процессе. Пример – L1-регуляризация (Lasso) в линейных моделях: она зануляет вес некоторых признаков, effectively выбирая их. Другой пример – деревья решений и их ансамбли: они вычисляют <em>feature importance</em> (важность признаков) на основе уменьшения ошибки/импурити при разбиениях. Признаки с высокой важностью сильнее влияют на предсказание, и можно отбирать по порогу этой важности.</li><br>  <li><strong>Перестановочная важность:</strong> уже после того, как модель обучена, можно оценить вклад признака, перемешивая его значения и измеряя, насколько упало качество модели. Если перестановка какого-то признака сильно ухудшает метрику, значит признак значимый. Этот метод не меняет модель, а проверяет её зависимость от каждого признака.</li><br>  <li><strong>Анализ признаков вручную и по знанию предмета:</strong> иногда понимание предметной области позволяет заранее отбросить нерелевантные признаки или выбрать наиболее перспективные. Это не автоматический, но важный путь отбора признаков – экспертное мнение. Также можно использовать методы снижения размерности (PCA и др.), чтобы увидеть, какие признаки больше влияют на главные компоненты, хотя это скорее трансформация, чем отбор непосредственно.</li><br></ul><br>На практике часто комбинируют подходы: сначала фильтрационно убрать заведомо слабые признаки, затем применить встроенный метод (например, обучить случайный лес и посмотреть на важности), и/или выполнить RFE. Главное – исходить из того, чтобы оставить признаки, дающие наибольший вклад в прогноз, и убрать шумовые/избыточные."
  },
  {
    "id": 26,
    "module": "Теория",
    "question": "Опишите стратегии подбора оптимальных гиперпараметров моделей машинного обучения",
    "answer": "Чтобы найти лучшие гиперпараметры модели, используют разные стратегии поиска:<br><ul><br>  <li><strong>Grid Search (полный перебор):</strong> задаётся набор возможных значений для каждого гиперпараметра и затем перебираются все комбинации в сетке. Для каждой комбинации модель обучается (обычно с перекрёстной проверкой), и выбирается комбинация с наилучшей метрикой. Этот метод прост, но при большом числе параметров и значений становится очень затратным.</li><br>  <li><strong>Random Search:</strong> вместо полного перебора, пробуют случайные комбинации гиперпараметров в заданных диапазонах. Исследования показали, что случайный поиск может быстрее найти хорошее решение, т.к. не тратит время на заведомо слабые комбинации равномерно. Мы заранее выбираем число итераций, и случайным образом выбираем значение каждого гиперпараметра по распределению. В итоге, за то же время можно покрыть больше разнообразных вариантов, чем сеткой.</li><br>  <li><strong>Байесовская оптимизация:</strong> более умный подход. Строится модель (например, гауссовский процесс) зависимости качества от гиперпараметров по уже проверенным точкам, и новые комбинации выбираются не случайно, а там, где по прогнозу может быть лучшее качество (с учётом неопределённости). Алгоритмы типа Tree-structured Parzen Estimator (TPE), используемые в Hyperopt, Optuna, или GaussianProcess (в scikit-optimize), постепенно приближаются к оптимуму с меньшим числом запусков модели.</li><br>  <li><strong>Эволюционные и другие методы:</strong> существуют и генетические алгоритмы для подбора параметров, и метод Hyperband (комбинация случайного поиска с адаптивным выделением ресурсов, когда негодные варианты отбрасываются раньше). Эти подходы пытаются более эффективно исследовать пространство параметров, особенно если параметры много и функция оценки шумная.</li><br>  <li><strong>Настройка с экспертом и поэтапно:</strong> иногда гиперпараметры подбирают вручную, используя знание алгоритма. Например, сначала настроить самый важный параметр, потом подстроить второстепенные. Или комбинировать: провести грубый поиск по широкому диапазону (random), затем сделать более мелкую сетку вокруг лучших областей. В любом случае, используют перекрёстную валидацию или выделенную валидацию, чтобы оценить качество каждой комбинации честно.</li><br></ul><br>Стратегии часто комбинируют: например, начать с random search, а затем Bayesian на лучшем интервале. Главное – автоматизировать поиск, чтобы не настраивать всё вручную, и избежать подборки под тестовую выборку (всегда держать независимый тест для финальной проверки)."
  },
  {
    "id": 27,
    "module": "Теория",
    "question": "Если вы собрались заниматься stacking / blending, то какие модели лучше в этом всём использовать?",
    "answer": "В стекинге (блендинге) успех во многом зависит от разнообразия и качества базовых моделей и правильно выбранной мета-модели:<br><ul><br>  <li><strong>Разнообразные базовые модели:</strong> стоит включать несколько моделей разных типов, чтобы они дополняли друг друга. Например, в ансамбле могут быть градиентный бустинг деревьев, случайный лес, нейронная сеть, k-ближайших соседей, SVM, логистическая регрессия – то есть комбинация алгоритмов, которые ошибаются по-разному. Главное, чтобы их прогнозы не были полностью коррелированы. Также можно брать одну модель, но с разными гиперпараметрами или обученную на разных признаках – тоже даст разнообразие.</li><br>  <li><strong>Метамодель (второй уровень):</strong> обычно берут простую модель для окончательного объединения предсказаний. Часто это логистическая регрессия (для классификации) или линейная регрессия (для регрессии), иногда более сложный алгоритм типа небольшого дерева решений. Простая метамодель снижает риск переобучиться на выходах базовых моделей и обычно хорошо справляется с объединением. Её задача – научиться, каким моделям доверять больше в каких случаях.</li><br></ul><br>Таким образом, лучше использовать смесь сильных, но разных по природе алгоритмов в качестве первого уровня, а в качестве второго уровня – простой объединяющий алгоритм. Например, в стеке могут одновременно участвовать CatBoost, нейронная сеть и SVM как базовые, а мета-learner соберёт их прогнозы посредством логистической регрессии. Такое сочетание вероятно даст более устойчивый результат, чем любая отдельная модель."
  },
  {
    "id": 28,
    "module": "Теория",
    "question": "Чем подход Бэггинг отличается от метода Головования?",
    "answer": "Оба метода – способы объединения нескольких моделей (ансамбли), но отличаются тем, как эти модели получены:<br><ul><br>  <li><strong>Бэггинг (Bootstrap Aggregating):</strong> предполагает обучение множества моделей одного типа на разных подвыборках данных. Обычно берутся бутстрап-выборки (случайные выборки с возвращением) из исходного набора, и на каждой обучается, скажем, одно и то же базовое алгоритмическое дерево. В результате получается ансамбль равноправных моделей, чьи предсказания усредняются (для регрессии) или голосуются (для классификации). Бэггинг уменьшает разброс модели: усреднение нескольких переобученных моделей даёт более стабильный результат.</li><br>  <li><strong>Ансамбль голосованием (Voting ensemble):</strong> подразумевает, что у нас есть несколько разных моделей (чаще всего различных алгоритмов или разной природы) и мы комбинируем их выходы. Например, у вас есть логистическая регрессия, решающее дерево и SVM – у каждого своё мнение о классе, и финальный класс берётся большинством голосов (или усреднением вероятностей). Важно, что здесь модели могут быть обучены на одних и тех же данных (без бутстрапа) – разнообразие достигается за счёт разной модели или разных настроек. Voting обычно не предполагает, что модели обязательно одного типа или что данные были ресемплированы.</li><br></ul><br>Кратко: при бэггинге мы создаём множество моделей, изменяя данные (каждая на случайной выборке, модели как правило одинаковые), а при голосовании берём уже готовые разные модели и объединяем их решения. Бэггинг – способ снизить дисперсию алгоритма, а голосование – способ воспользоваться преимуществами разных методов одновременно."
  },
  {
    "id": 29,
    "module": "Теория",
    "question": "В чем отличие задачи обнаружения выбросов (outlier detection) и обнаружение новизны (novelty detection) в контексте задачи поиска аномалий?",
    "answer": "Различие между <strong>outlier detection</strong> и <strong>novelty detection</strong> заключается в том, имеются ли аномальные объекты в обучающей выборке и как мы к ним подходим:<br><ul><br>  <li><strong>Обнаружение выбросов (outlier detection):</strong> предполагает, что в доступных данных (в том числе обучающих) могут присутствовать выбросы – то есть точки, выбивающиеся из общего распределения. Алгоритм пытается найти необычные объекты в этом же наборе, который ему дан. По сути, это полностью непросупервайзед (безметочный) подход: модель строит представление о структуре данных и помечает те точки, которые не вписываются. Примеры: Local Outlier Factor, Isolation Forest могут использоваться так, чтобы сразу выявлять выбросы в обучающем множестве.</li><br>  <li><strong>Обнаружение новизны (novelty detection):</strong> означает, что обучающую выборку мы считаем чистой, содержащей только нормальные данные (без аномалий). Модель учится описывать \"нормальное\" поведение. А уже затем новые поступающие объекты проверяются на принадлежность к этому нормальному распределению – и если не принадлежат, считаются аномалиями (новыми, ранее не виданными образцами). То есть обучение и поиск аномалий разделены по времени: сначала на нормальных данных, потом проверка новых. Пример: One-Class SVM обычно тренируется на нормальных объектах и потом выявляет новизну в тестовых.</li><br></ul><br>Таким образом, разница в наличии аномалий при обучении: outlier detection – мы сразу выявляем странные точки внутри данного набора, novelty detection – мы готовим модель на хорошем наборе и ищем аномальные случаи только в будущем потоке данных."
  },
  {
    "id": 30,
    "module": "Теория",
    "question": "Почему самый базовый метод поиска аномалий в scikit-learn носит название EllepticEnvelope?",
    "answer": "Метод <strong>EllipticEnvelope</strong> так назван, потому что он пытается обрисовать эллипсоидальную \"оболочку\" вокруг данных, в пределах которой точки считаются нормальными. Этот алгоритм основывается на предположении, что распределение данных близко к многомерному нормальному. Он оценивает ковариационную матрицу и среднее выборки (возможно, с учетом робастности) и определяет эллипсоид (называемый доверительным эллипсом) покрывающий основную массу данных. Все, что лежит внутри эллипса – нормально, а выбросы – снаружи. <br>Таким образом, название отражает суть: строится эллиптическая граница (Envelope) вокруг облака точек. Это действительно один из самых базовых подходов: если данные примерно гауссовские, то границы уровня плотности – эллипсы, и задача сводится к поиску такого эллипса, который охватывает заданную долю точек (например, 98%). Всё вне – аномалии."
  },
  {
    "id": 31,
    "module": "Теория",
    "question": "LocalOutlierFactor: почему при вычислении Reachability distance получается не симметричная матрица?",
    "answer": "В методе LOF (Local Outlier Factor) <em>reachability distance</em> определяется не просто как расстояние между точками, а с учётом радиуса соседей одной из точек. Конкретно, <strong>reachability-dist<sub>k</sub>(A, B)</strong> = max{ k-dist(B), dist(A, B) }, где k-dist(B) – расстояние от точки B до её k-го соседа. <br>Из-за этого reachability расстояние несимметрично: reachability-dist(A, B) и reachability-dist(B, A) – разные величины, ведь в одном случае берётся k-дистанция точки B, а в другом – точки A. Например, если точка B имеет далёких соседей (большой радиус k-dist), то reach-dist(A,B) примет это большое значение (ограничится им), а обратное reach-dist(B,A) может быть совсем другим (ограничится k-dist(A) если оно иное). <br>Проще говоря, расстояние достижимости – направленное: оно показывает, насколько A доступен из B в пределах плотности вокруг B. В LOF так задумано, чтобы сравнивать плотности: локальная плотность A оценивается относительно плотности его соседей. Поэтому матрица таких расстояний получается нессимметричной."
  },
  {
    "id": 32,
    "module": "Теория",
    "question": "LocalOutlierFactor: на что влияет показатель k-соседей?",
    "answer": "Параметр k в Local Outlier Factor определяет, сколько ближайших соседей считать при оценке локальной плотности. От него зависит \"масштаб\" окрестности, на фоне которой точка будет оцениваться как аномальная или нет.<br><ul><br>  <li>При небольшом k рассматривается очень локальное окружение. Это значит, что LOF будет чувствителен к мелким группам: даже если точка образует маленький кластер из нескольких соседей, при маленьком k она может не считаться выбросом. Но если k слишком мал, алгоритм может отмечать как выбросы те точки, которые просто слегка изолированы, хоть и нормальны (шум воспринимается как сильный outlier).</li><br>  <li>При большом k берётся широкая окрестность. Тогда плотность точки сравнивается с плотностью большого региона. Это сглаживает локальные вариации: небольшие кластеры могут быть пропущены и помечены как \"разреженные\" (аномальные), ведь в сравнении с большим соседством они отличаются. Зато выбор большого k делает алгоритм более стабильным и менее чувствительным к мелкому шуму.</li><br></ul><br>Таким образом, k влияет на баланс между чувствительностью к локальным выбросам и общим сглаживанием: нужно выбирать его, исходя из предположаемого размера типичных кластеров в данных. Например, если ожидается, что аномалии будут проявляться как одиночки среди минимум десятков нормальных точек, то k имеет смысл брать порядка этих десятков."
  },
  {
    "id": 33,
    "module": "Теория",
    "question": "Одноклассовый метод опорных векторов: почему отделять выборку от нуля это не бессмысленно?",
    "answer": "One-Class SVM (одноклассовый SVM) кажется необычным: он пытается отделить все обучающие данные от начала координат (нуль-вектора) максимально далеко. Смысл в том, что таким образом алгоритм фактически вычерчивает границу вокруг наших данных в пространстве признаков. <br>Представим, что все \"нормальные\" данные – это точки, которые алгоритм видел при обучении. One-Class SVM старается найти \"пузырь\" – область в пространстве, которая содержит эти точки и отделена от остального пространства. Задача формулируется как поиск гиперплоскости, отделяющей данные от начала координат, но в ядровом пространстве это эквивалентно построению некоторой нелинейной оболочки вокруг данных. Таким образом, отделять выборку от нуля – это способ научиться распознавать, где заканчивается область нормальных данных. Всё, что окажется по другую сторону гиперплоскости (ближе к нулю), будет считаться аномалией. Это вовсе не бессмысленно: наоборот, так мы учим модель отличать \"то, что похоже на обучающие данные\" от \"того, что сильно отличается\" (представленного условно нулём). Именно поэтому One-Class SVM и применяется для задачи novelty detection – он обучается на одном классе (норме) и затем различает норму и \"не-норму\"."
  },
  {
    "id": 34,
    "module": "Теория",
    "question": "IsolationForest: как строятся деревья для поиска аномалий?",
    "answer": "Isolation Forest строит специальные деревья таким образом, чтобы \"случайно\" изолировать каждую точку. Процесс для каждого дерева выглядит так:<br><ol><br>  <li>Выбирается случайно признак и на нём случайно выбирается порог (между мин и макс значением этого признака у текущих точек).</li><br>  <li>Данные разделяются этим порогом на две части – те, у которых значение признака меньше порога, и те, у которых больше (как обычное бинарное разделение в дереве).</li><br>  <li>Затем для каждой получившейся части рекурсивно повторяется процесс: снова случайно выбирается признак и порог, и выполняется разделение.</li><br>  <li>Дерево растёт до тех пор, пока в узле не останется одна точка (она изолирована) или пока не достигнута максимальная глубина.</li><br></ol><br><p>Несколько таких случайных деревьев (лес) образуют модель. Идея в том, что аномальные точки обычно находятся \"особняком\" – у них какие-то уникальные, экстремальные значения признаков. Поэтому случайное разделение пространства вероятнее очень быстро отсечёт такую точку от остальных. Она попадёт в отдельный узел спустя мало разбиений (короткий путь в дереве). А вот нормальные объекты, которые сконцентрированы в массовых кластерах, будут отделяться от других постепенно, им потребуется больше случайных разбиений, то есть у них длина пути до изоляции будет больше. Так и строятся деревья Isolation Forest: без какого-либо критерия информативности, чисто случайно разрезая пространство признаков, что в среднем сильно изолирует выбросы раньше остальных.</p>"
  },
  {
    "id": 35,
    "module": "Теория",
    "question": "Почему IsolationForest такой быстрый (https://www.lamda.nju.edu.cn/publication/icdm08b.pdf)?",
    "answer": "Isolation Forest выигрывает в скорости за счёт своей простоты и особенностей реализации:<br><ul><br>  <li><strong>Линейная сложность:</strong> алгоритм по сути работает за O(n) на построение (плюс умноженное на число деревьев), если брать фиксированный размер выборки на одно дерево. Он не считает расстояния между всеми парами точек и не оптимизирует сложную цель – он просто случайно разбивает данные. Каждая точка проходит через несколько случайных разделений (глубина дерева), обычно глубина ограничена логарифмом от числа точек. В итоге затраты ~ O(n log n) или даже O(n) при ограниченной глубине на дерево.</li><br>  <li><strong>Использование подвыборок:</strong> Isolation Forest часто обучают на небольших подвыборках данных (например, по 256 точек на дерево). Это ещё ускоряет работу и не сильно влияет на качество обнаружения выбросов, так как большего количества точек на одно дерево обычно не нужно. Благодаря этому время обучения масштабируется практически линейно с размером данных, с небольшим коэффициентом.</li><br>  <li><strong>Отсутствие дорогостоящих вычислений:</strong> В алгоритме нет вычисления ковариаций, градиентов или сортировки всех точек по каждому признаку при каждом разбиении (как бывает в детерминированных деревьях). Берётся случайный порог – значит, никакой сложной логики или вычисления критериев не выполняется. Это минимизирует накладные расходы.</li><br>  <li><strong>Параллелизм:</strong> Поскольку каждое дерево строится независимо, их можно строить одновременно на нескольких ядрах процессора. Это позволяет ещё быстрее обучать лес на больших данных, распределяя нагрузку.</li><br></ul><br>Таким образом, IsolationForest – алгоритм с очень низкими затратами на каждое разбиение (они случайны и просты) и с возможностью обрабатывать только часть данных на дерево. Поэтому он масштабируется на большие выборки гораздо лучше, чем методы, требующие глобальных вычислений вроде плотностей или расстояний между каждой парой точек."
  }
]